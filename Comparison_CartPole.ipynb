{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "expected sequence of length 4 at dim 1 (got 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 210\u001b[0m\n\u001b[0;32m    207\u001b[0m buffer \u001b[38;5;241m=\u001b[39m ReplayBuffer(capacity\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100000\u001b[39m)\n\u001b[0;32m    209\u001b[0m \u001b[38;5;66;03m# Train the D4PG agent and get rewards\u001b[39;00m\n\u001b[1;32m--> 210\u001b[0m rewards \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_d4pg\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcritic\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_actor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_critic\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactor_optimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcritic_optimizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    212\u001b[0m \u001b[38;5;66;03m# Calculate moving average rewards\u001b[39;00m\n\u001b[0;32m    213\u001b[0m window_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m50\u001b[39m\n",
      "Cell \u001b[1;32mIn[8], line 137\u001b[0m, in \u001b[0;36mtrain_d4pg\u001b[1;34m(env, actor, critic, target_actor, target_critic, buffer, actor_optimizer, critic_optimizer, n_episodes, batch_size)\u001b[0m\n\u001b[0;32m    134\u001b[0m episode_reward \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m    136\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m--> 137\u001b[0m     state_tensor \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m    138\u001b[0m     action \u001b[38;5;241m=\u001b[39m actor(state_tensor)\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy()[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    139\u001b[0m     next_state, reward, done, _ \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action)\n",
      "\u001b[1;31mValueError\u001b[0m: expected sequence of length 4 at dim 1 (got 0)"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from collections import deque\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# Define the Actor Network\n",
    "class Actor(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super(Actor, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(state_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, action_dim),\n",
    "            nn.Sigmoid(),  # Logistic output\n",
    "        )\n",
    "\n",
    "    def forward(self, state):\n",
    "        return self.net(state)\n",
    "\n",
    "\n",
    "# Define the Critic Network\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super(Critic, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(state_dim + action_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, state, action):\n",
    "        return self.net(torch.cat([state, action], dim=1))\n",
    "\n",
    "\n",
    "# Replay Buffer\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "\n",
    "    def add(self, transition):\n",
    "        self.buffer.append(transition)\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        batch = random.sample(self.buffer, batch_size)\n",
    "        states, actions, rewards, next_states, dones = zip(*batch)\n",
    "        return (\n",
    "            torch.tensor(states, dtype=torch.float32),\n",
    "            torch.tensor(actions, dtype=torch.float32),\n",
    "            torch.tensor(rewards, dtype=torch.float32).unsqueeze(1),\n",
    "            torch.tensor(next_states, dtype=torch.float32),\n",
    "            torch.tensor(dones, dtype=torch.float32).unsqueeze(1),\n",
    "        )\n",
    "\n",
    "    def size(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "\n",
    "# Train RL Algorithm\n",
    "def train_rl_algorithm(algorithm, env, actor, critic, target_actor, target_critic, buffer, actor_optimizer,\n",
    "                       critic_optimizer, n_episodes=500, batch_size=64, gamma=0.99, tau=0.005, reward_threshold=300):\n",
    "    rewards = []\n",
    "    steps_to_convergence = None\n",
    "\n",
    "    for episode in range(n_episodes):\n",
    "        state = env.reset()\n",
    "        episode_reward = 0\n",
    "        while True:\n",
    "            state_tensor = torch.tensor(state, dtype=torch.float32).unsqueeze(0)\n",
    "            action = actor(state_tensor).detach().numpy()[0]\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            buffer.add((state, action, reward, next_state, done))\n",
    "\n",
    "            if buffer.size() > batch_size:\n",
    "                states, actions, rewards_batch, next_states, dones = buffer.sample(batch_size)\n",
    "\n",
    "                # Update Critic\n",
    "                target_actions = target_actor(next_states)\n",
    "                target_q = rewards_batch + gamma * (1 - dones) * target_critic(next_states, target_actions).detach()\n",
    "                critic_loss = nn.MSELoss()(critic(states, actions), target_q)\n",
    "                critic_optimizer.zero_grad()\n",
    "                critic_loss.backward()\n",
    "                critic_optimizer.step()\n",
    "\n",
    "                # Update Actor\n",
    "                actor_loss = -critic(states, actor(states)).mean()\n",
    "                actor_optimizer.zero_grad()\n",
    "                actor_loss.backward()\n",
    "                actor_optimizer.step()\n",
    "\n",
    "                # Soft update\n",
    "                for target_param, param in zip(target_actor.parameters(), actor.parameters()):\n",
    "                    target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)\n",
    "                for target_param, param in zip(target_critic.parameters(), critic.parameters()):\n",
    "                    target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)\n",
    "\n",
    "            state = next_state\n",
    "            episode_reward += reward\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        rewards.append(episode_reward)\n",
    "        if steps_to_convergence is None and episode_reward >= reward_threshold:\n",
    "            steps_to_convergence = episode\n",
    "\n",
    "    return rewards, steps_to_convergence\n",
    "\n",
    "\n",
    "# Plot Metrics\n",
    "def calculate_metrics_and_plot(algorithms_data, window_size=50):\n",
    "    # Cumulative Rewards vs Episodes\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    for algo_name, data in algorithms_data.items():\n",
    "        rewards = data[\"rewards\"]\n",
    "        smoothed_rewards = np.convolve(rewards, np.ones(window_size) / window_size, mode=\"valid\")\n",
    "        plt.plot(smoothed_rewards, label=f\"{algo_name} (Moving Avg)\")\n",
    "\n",
    "    plt.xlabel(\"Episodes\")\n",
    "    plt.ylabel(\"Reward\")\n",
    "    plt.title(\"Cumulative Rewards vs Episodes\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    # Convergence Speed and Sample Efficiency\n",
    "    algo_names = []\n",
    "    steps_to_convergence = []\n",
    "    for algo_name, data in algorithms_data.items():\n",
    "        algo_names.append(algo_name)\n",
    "        steps_to_convergence.append(data[\"steps_to_convergence\"] or len(data[\"rewards\"]))\n",
    "\n",
    "    plt.bar(algo_names, steps_to_convergence, color=\"blue\")\n",
    "    plt.title(\"Convergence Speed\")\n",
    "    plt.ylabel(\"Steps to Convergence\")\n",
    "    plt.show()\n",
    "\n",
    "    # Stability (Standard Deviation of Rewards)\n",
    "    std_devs = [np.std(data[\"rewards\"]) for data in algorithms_data.values()]\n",
    "    plt.bar(algo_names, std_devs, color=\"orange\")\n",
    "    plt.title(\"Stability (Standard Deviation of Rewards)\")\n",
    "    plt.ylabel(\"Standard Deviation\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Main\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = 1  # Continuous action\n",
    "\n",
    "algorithms_data = {}\n",
    "for algorithm in [\"DDPG\", \"D4PG\", \"TD3\", \"SAC\"]:\n",
    "    actor = Actor(state_dim, action_dim)\n",
    "    critic = Critic(state_dim, action_dim)\n",
    "    target_actor = Actor(state_dim, action_dim)\n",
    "    target_critic = Critic(state_dim, action_dim)\n",
    "    target_actor.load_state_dict(actor.state_dict())\n",
    "    target_critic.load_state_dict(critic.state_dict())\n",
    "\n",
    "    actor_optimizer = optim.Adam(actor.parameters(), lr=1e-3)\n",
    "    critic_optimizer = optim.Adam(critic.parameters(), lr=1e-3)\n",
    "\n",
    "    buffer = ReplayBuffer(capacity=100000)\n",
    "\n",
    "    rewards, steps_to_convergence = train_rl_algorithm(\n",
    "        algorithm=algorithm,\n",
    "        env=env,\n",
    "        actor=actor,\n",
    "        critic=critic,\n",
    "        target_actor=target_actor,\n",
    "        target_critic=target_critic,\n",
    "        buffer=buffer,\n",
    "        actor_optimizer=actor_optimizer,\n",
    "        critic_optimizer=critic_optimizer,\n",
    "        n_episodes=500,\n",
    "        batch_size=64,\n",
    "        gamma=0.99,\n",
    "        tau=0.005,\n",
    "        reward_threshold=300,\n",
    "    )\n",
    "\n",
    "    algorithms_data[algorithm] = {\"rewards\": rewards, \"steps_to_convergence\": steps_to_convergence}\n",
    "\n",
    "# Visualize Metrics\n",
    "calculate_metrics_and_plot(algorithms_data)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
